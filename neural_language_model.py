# -*- coding: utf-8 -*-
"""NLM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BIXixCphGyq3wdrsTM7Ir-reYPiMv-eX
"""

import tensorflow as tf
import tensorflow.keras.callbacks
import tensorflow.keras.layers
import tensorflow.keras.callbacks

from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.model_selection import train_test_split
import numpy as np
import os
import re

def basic_tokenizer(input_file, output_file):
    # Open the input file for reading
    with open(input_file, 'r') as f:
        text = f.read()

    url_pattern = re.compile(r'https?://\S+')
    hashtag_pattern = re.compile(r'#\w+')
    mention_pattern = re.compile(r'@\w+')

    processed_corpus = re.sub(url_pattern, '<URL>', text)
    processed_corpus = re.sub(hashtag_pattern, '<HASHTAG>', processed_corpus)
    processed_corpus = re.sub(mention_pattern, '<MENTION>', processed_corpus)

    # Remove punctuation and split into tokens
    tokens = re.findall(r'\w+', processed_corpus.lower())

    # Open the output file for writing
    with open(output_file, 'w') as f:
        # Write each token to a new line in the output file
        for token in tokens:
            f.write('"' + token + '"\n')

# Define hyperparameters
max_vocab_size = 10000
max_sequence_length = 50
embedding_size = 64
lstm_size = 128
batch_size = 256
num_epochs = 5
learning_rate = 0.005
dropout_rate = 0.5
train_frac = 0.7
dev_frac = 0.15
test_frac = 0.15

# Basic tokenization of the raw text files
input_file = 'PnP_raw.txt'
output_file = 'PnP_tokenized.txt'
basic_tokenizer(input_file, output_file)
input_file = 'Ulysses_raw.txt'
output_file = 'Ulysses_tokenized.txt'
basic_tokenizer(input_file, output_file)

# Load tokenized datasets
def load_data(filepath):
    with open(filepath, 'r') as f:
        lines = f.read().split('\n')
    # Remove any empty lines
    lines = [line for line in lines if line]
    # Convert each line into a list of tokens
    sequences = [line.split() for line in lines]
    return sequences

PnP_sequences = load_data('PnP_tokenized.txt')
Ulysses_sequences = load_data('Ulysses_tokenized.txt')

# Create word-to-index mapping
def create_word_to_index_mapping(sequences):
    word_counts = {}
    for sequence in sequences:
        for word in sequence:
            if word not in word_counts:
                word_counts[word] = 1
            else:
                word_counts[word] += 1
    # Sort words in descending order of frequency
    sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)
    # Add special tokens to the beginning
    sorted_words = ['<PAD>', '<UNK>', '<s>', '</s>'] + sorted_words
    # Truncate vocabulary to max_vocab_size
    sorted_words = sorted_words[:max_vocab_size]
    word_to_index = {word: index for index, word in enumerate(sorted_words)}
    return word_to_index

PnP_word_to_index = create_word_to_index_mapping(PnP_sequences)
Ulysses_word_to_index = create_word_to_index_mapping(Ulysses_sequences)

def count_words(data):
    word_counts = {}
    for sequence in data:
        for word in sequence:
            if word not in word_counts:
                word_counts[word] = 1
            else:
                word_counts[word] += 1
    return word_counts

def create_vocabulary(word_counts, max_vocab_size):
    # Sort words in descending order of frequency
    sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)
    # Add special tokens to the beginning
    sorted_words = ['<PAD>', '<UNK>', '<s>', '</s>'] + sorted_words
    # Truncate vocabulary to max_vocab_size
    sorted_words = sorted_words[:max_vocab_size]
    word_to_index = {word: index for index, word in enumerate(sorted_words)}
    index_to_word = {index: word for word, index in word_to_index.items()}
    return word_to_index, index_to_word

# Convert words to indices in the sequences
def convert_to_indices(sequences, word_to_index):
    unk_index = word_to_index['<UNK>']
    sequences_as_indices = []
    for sequence in sequences:
        sequence_as_indices = []
        for word in sequence:
            if word in word_to_index:
                sequence_as_indices.append(word_to_index[word])
            else:
                sequence_as_indices.append(unk_index)
        sequences_as_indices.append(sequence_as_indices)
    return sequences_as_indices

#Convert words to indices in the sequences
PnP_sequences_as_indices = convert_to_indices(PnP_sequences, PnP_word_to_index)
Ulysses_sequences_as_indices = convert_to_indices(Ulysses_sequences, Ulysses_word_to_index)

# Pad sequences to max_sequence_length
PnP_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(
PnP_sequences_as_indices, maxlen=max_sequence_length, padding='post', truncating='post')
Ulysses_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(
Ulysses_sequences_as_indices, maxlen=max_sequence_length, padding='post', truncating='post')

# Concatenate the two datasets
all_sequences = np.concatenate((PnP_sequences_padded, Ulysses_sequences_padded), axis=0)

# Create labels
PnP_labels = np.zeros((len(PnP_sequences_padded),), dtype=np.int32)
Ulysses_labels = np.ones((len(Ulysses_sequences_padded),), dtype=np.int32)
all_labels = np.concatenate((PnP_labels, Ulysses_labels), axis=0)

# Split into train, dev, and test sets
X_train, X_test, y_train, y_test = train_test_split(all_sequences, all_labels, test_size=test_frac, stratify=all_labels)
X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=dev_frac/(train_frac+dev_frac), stratify=y_train)

# Define the model architecture
model = Sequential([
Embedding(max_vocab_size, embedding_size, input_length=max_sequence_length),
LSTM(lstm_size, dropout=dropout_rate),
Dense(1, activation='sigmoid')
])

# Compile the model
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, validation_data=(X_dev, y_dev), batch_size=batch_size, epochs=num_epochs)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)
print(f'Test loss: {test_loss}, test accuracy: {test_accuracy}')

# Define the checkpoint callback
checkpoint_path = "model_checkpoint/checkpoint"
checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,
                                      save_weights_only=True,
                                      save_freq='epoch')

model.save('my_model.h5')

# Define a function to calculate perplexity
def calculate_perplexity(model, sequences):
    # Calculate the negative log likelihood of each sequence
    log_likelihoods = []
    for sequence in sequences:
        padded_sequence = sequence + [PnP_word_to_index['<PAD>']] * (max_sequence_length - len(sequence))
        padded_sequence = np.array(padded_sequence).reshape(1, -1)
        logits = model.predict(padded_sequence)
        log_likelihood = 0
        for i in range(len(sequence)):
            log_likelihood += np.log2(logits[0, i, sequence[i]])
        log_likelihoods.append(log_likelihood)
    # Calculate the perplexity score
    perplexities = [2**(-log_likelihood/len(sequence)) for log_likelihood, sequence in zip(log_likelihoods, sequences)]
    return perplexities

# Load the trained model
model = tf.keras.models.load_model('my_model.h5')

# Calculate the perplexity scores
PnP_train_perplexities = calculate_perplexity(model, PnP_sequences_as_indices)
Ulysses_train_perplexities = calculate_perplexity(model, Ulysses_sequences_as_indices)

# Calculate the average perplexity score
avg_PnP_train_perplexity = sum(PnP_train_perplexities) / len(PnP_train_perplexities)
avg_Ulysses_train_perplexity = sum(Ulysses_train_perplexities) / len(Ulysses_train_perplexities)

print("Average Perplexity Score on Train Set for P&P: ", avg_PnP_train_perplexity)
print("Average Perplexity Score on Train Set for Ulysses: ", avg_Ulysses_train_perplexity)

import matplotlib.pyplot as plt

# Create the data array
loss = [0.6236, 0.6230, 0.6229, 0.6229, 0.6229]
accuracy = [0.6854,0.6854,0.6854,0.6854,0.6854]
val_loss = [0.6231, 0.6227, 0.6228, 0.6227, 0.6231]
val_accuracy = [0.6854,0.6854,0.6854,0.6854,0.6854]

# Plot the data as a line graph
plt.plot(loss)
plt.plot(accuracy)
plt.plot(val_loss)
plt.plot(val_accuracy)

# Add title and axis labels
plt.title('Line Graph of Data')
plt.xlabel('X-axis Label')
plt.ylabel('Y-axis Label')

# Display the plot
plt.show()

